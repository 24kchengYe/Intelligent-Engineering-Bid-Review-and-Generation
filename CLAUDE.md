# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a Streamlit-based **Intelligent Bidding Document Review and Technical Proposal Generation System** (æ™ºèƒ½æ ‡ä¹¦å®¡æŸ¥ç³»ç»Ÿ/æ™ºæ ‡é¢†èˆª) that uses Claude AI to:
1. Parse and analyze engineering bidding documents with **7-category structured analysis**
2. **OCR support for scanned PDFs** (automatic text recognition)
3. **Structured table extraction** from PDF documents
4. **National/local standards management** (upload and reference standard documents)
5. Extract evaluation criteria automatically
6. Generate technical proposal outlines based on evaluation criteria
7. Generate technical proposal sections step-by-step

The system is designed for internal use and operates as a standalone web application without separate frontend/backend architecture.

## New Features (v1.1.0)

### ğŸ” OCR for Scanned PDFs
- Automatically detects scanned PDF pages (pages with no text layer)
- Uses **RapidOCR** engine for offline text recognition
- Displays OCR statistics in file upload interface
- Confidence filtering (â‰¥0.5) to ensure quality

### ğŸ“Š Structured Table Extraction
- Extracts tables from PDF as 2D arrays (not plain text)
- Preserves table structure for better analysis
- Displays table count in metadata

### ğŸ“š Standards Library Management
- **New Tab**: "å›½æ ‡ç®¡ç†" for managing standard documents
- Upload national/industry/local standards (GB/JGJ/CJJ/DB...)
- Automatic standard code extraction (e.g., GB50500-2013)
- Categorization: å›½å®¶æ ‡å‡†/è¡Œä¸šæ ‡å‡†/åœ°æ–¹æ ‡å‡†
- Search and filter capabilities
- Independent storage (not tied to project records)

## Development Commands

### Setup
```bash
# Install dependencies
pip install -r requirements.txt

# Configure API key (required before first run)
cp .env.example .env
# Edit .env and add your ANTHROPIC_API_KEY
```

### Running the Application
```bash
# Start the Streamlit application
streamlit run app.py

# Quick start (Windows) - Recommended
å¯åŠ¨ç³»ç»Ÿ.bat

# Quick start (Mac/Linux)
./start.sh
```

### Testing
```bash
# Verify environment configuration
python test_setup.py

# Test AI prompts
python test_prompts.py
```

## Core Architecture

### Three-Phase Workflow
The system implements a sequential workflow with strict dependencies:

#### **Phase 1: File Upload** (`file_upload_tab`)
- User uploads bidding documents (PDF/Word/Excel) - All 6 categories are optional
- Files are saved to `database/` folder with timestamped names
- `DocumentParser` immediately extracts text content with encoding support for Chinese/English/numbers
- File metadata and content stored in `st.session_state`
- Record saved to database with status='draft'
- UI displays **6 file categories** in two sections:
  - **æ‹›æ ‡æ–‡ä»¶** (2 categories): æ‹›æ ‡æ–‡ä»¶æ­£æ–‡, æŠ€æœ¯è¦æ±‚é™„ä»¶
  - **æ‹›æ ‡æ–‡ä»¶é™„ä»¶** (4 categories): å·¥ç¨‹é‡æ¸…å•, è¯„å®¡æ ‡å‡†é™„ä»¶, æ–½å·¥è®¾è®¡è¯´æ˜, æ–¹æ¡ˆå»ºè®®ä¹¦é™„ä»¶

#### **Phase 2: Structured Analysis** (`analysis_tab`)
**Requires**: Uploaded files in session state

**Step 1: 7-Category Structured Parsing**
- Calls `ClaudeService.parse_bidding_document_structured(uploaded_files_content)`
- Uses `BIDDING_DOCUMENT_ANALYSIS_PROMPT` from `modules/prompts.py`
- Analyzes across 7 major categories:
  1. åŸºç¡€ä¿¡æ¯ (Basic Information)
  2. èµ„æ ¼è¦æ±‚ (Qualification Requirements)
  3. è¯„å®¡è¦æ±‚ (Evaluation Requirements)
  4. æŠ•æ ‡æ–‡ä»¶è¦æ±‚ (Bidding Document Requirements)
  5. æ— æ•ˆæ ‡ä¸åºŸæ ‡é¡¹ (Invalid & Rejected Bid Items)
  6. åº”æ ‡éœ€æäº¤æ–‡ä»¶ (Required Submission Documents)
  7. æ‹›æ ‡æ–‡ä»¶å®¡æŸ¥ (Bidding Document Review - includes 6 risk types + 5 fairness dimensions)
- Result saved to session state and database (status='analyzed')
- **UI**: Left/right pane layout showing parsed results + original document content

**Step 2: Evaluation Criteria Extraction**
- Automatically called after structured parsing
- Calls `ClaudeService.extract_evaluation_criteria(analysis_report)`
- Uses `EVALUATION_CRITERIA_EXTRACTION_PROMPT` from `modules/prompts.py`
- Extracts detailed evaluation criteria from analysis report
- Focuses on "æ–½å·¥ç»„ç»‡è®¾è®¡" (Construction Organization Design) evaluation factors
- Result saved to `session_state.evaluation_criteria` and `database.bidding_response` field

#### **Phase 3: Technical Proposal Generation** (`generation_tab`)
**Requires**: Completed analysis report + extracted evaluation criteria

**Step 1: Generate Technical Proposal Outline**
- Calls `ClaudeService.generate_technical_proposal_outline(project_requirements, evaluation_criteria)`
- Uses `TECHNICAL_PROPOSAL_OUTLINE_PROMPT` from `modules/prompts.py`
- Returns JSON structure with:
  - Multi-level outline (ä¸€ã€äºŒã€ä¸‰... / 1. 2. 3...)
  - Suggested word count for each section
  - Brief description of section requirements
- UI displays outline as expandable tree structure

**Step 2: Generate Sections Step-by-Step**
- User selects section from dropdown
- Calls `ClaudeService.generate_technical_proposal_section(section_title, word_count, section_requirements, project_info, evaluation_criteria)`
- Uses `TECHNICAL_PROPOSAL_SECTION_PROMPT` from `modules/prompts.py`
- Generated content saved to `session_state.generated_sections[section_title]`
- UI shows progress: "X/Y sections generated"
- User repeats until all sections complete

**Step 3: Export Complete Technical Proposal**
- Merges all generated sections into complete document
- Exports as Markdown or TXT format

### Session State Management
Critical session state variables that maintain workflow continuity:
- `current_record_id`: Links UI to database record
- `project_name`: User-defined project identifier
- `uploaded_files_info`: Dict[category, filename] - File metadata
- `uploaded_files_content`: Dict[category, text_content] - Required for analysis
- `files_processed`: Set[file_id] - For deduplication
- `analysis_report`: Text - 7-category structured parsing result
- `evaluation_criteria`: Text - Extracted evaluation criteria
- `technical_outline`: Dict - JSON outline structure
- `generated_sections`: Dict[section_title, content] - Generated proposal sections
- `bidding_response`: Text - Actually stores evaluation criteria (legacy field name)

When loading historical records (`load_record`), files must be re-parsed from `database/` directory since only filenames are stored in database.

### Service Initialization Pattern
Services are initialized using `@st.cache_resource` to persist across Streamlit reruns:
```python
ai_service, db_manager, document_parser, standards_manager = init_services()
```

This caching is critical - services should NOT be re-instantiated in individual functions.

**Services**:
- `ai_service` (ClaudeService): AI text generation
- `db_manager` (DatabaseManager): Project records database
- `document_parser` (DocumentParser): File parsing with OCR support
- `standards_manager` (StandardsManager): Standards library management

### File Category Configuration
File types are defined in `file_upload_tab` via `file_categories` list:
```python
file_categories = [
    # æ‹›æ ‡æ–‡ä»¶ (2 categories)
    {"name": "æ‹›æ ‡æ–‡ä»¶æ­£æ–‡", "types": ["pdf", "docx", "doc", "xlsx", "xls"], "category": "æ‹›æ ‡æ–‡ä»¶"},
    {"name": "æŠ€æœ¯è¦æ±‚é™„ä»¶", "types": ["pdf", "docx", "doc", "xlsx", "xls"], "category": "æ‹›æ ‡æ–‡ä»¶"},
    # æ‹›æ ‡æ–‡ä»¶é™„ä»¶ (4 categories)
    {"name": "å·¥ç¨‹é‡æ¸…å•", "types": ["pdf", "xlsx", "xls", "docx", "doc"], "category": "æ‹›æ ‡æ–‡ä»¶é™„ä»¶"},
    {"name": "è¯„å®¡æ ‡å‡†é™„ä»¶", "types": ["pdf", "docx", "doc", "xlsx", "xls"], "category": "æ‹›æ ‡æ–‡ä»¶é™„ä»¶"},
    {"name": "æ–½å·¥è®¾è®¡è¯´æ˜", "types": ["pdf", "docx", "doc", "dwg"], "category": "æ‹›æ ‡æ–‡ä»¶é™„ä»¶"},
    {"name": "æ–¹æ¡ˆå»ºè®®ä¹¦é™„ä»¶", "types": ["pdf", "docx", "doc", "xlsx", "xls"], "category": "æ‹›æ ‡æ–‡ä»¶é™„ä»¶"}
]
```

Key features:
- All file categories support multiple formats (PDF, Word, Excel)
- All 6 categories are **optional** by design
- Files are grouped into "æ‹›æ ‡æ–‡ä»¶" and "æ‹›æ ‡æ–‡ä»¶é™„ä»¶" sections in UI
- To add new categories, append to this list with appropriate `category` field

## Module Details

### `modules/document_parser.py` (~250 lines) â­ UPGRADED
- **DocumentParser**: Registry pattern with `supported_formats` dict mapping extensions to parser methods
- Each parser returns `{'content': str, 'metadata': dict}`

**PDF Parsing (Enhanced with OCR + Table Extraction)**:
- **Text Layer**: PyMuPDF (fitz) direct text extraction
- **Scanned Pages**: Automatic detection (`_is_scanned_page`) + RapidOCR recognition
  - Renders page at 2x DPI for better OCR quality
  - Filters OCR results by confidence (â‰¥0.5)
  - Returns `metadata['ocr_pages']` count
- **Table Extraction**: Uses PyMuPDF 1.23+ `find_tables()` API
  - Extracts tables as 2D arrays: `[[header1, header2], [data1, data2]]`
  - Converts to readable text format
  - Returns `metadata['tables']` with structured data
- **Methods**:
  - `_get_ocr_engine()`: Lazy OCR engine initialization
  - `_is_scanned_page(page)`: Detects if page has text layer (<30 chars = scanned)
  - `_ocr_page(page)`: OCR recognition pipeline
  - `_extract_tables(page)`: Table structure extraction
  - `_table_to_text(table_data)`: Format table as text

**Word**: Extracts paragraphs and tables separately, handles mixed Chinese/English text

**Excel**:
  - Processes all sheets with `pd.ExcelFile`
  - Uses `dtype=str` to preserve original formatting (numbers, Chinese, English)
  - Uses appropriate engine: openpyxl for .xlsx, xlrd for .xls
  - Returns metadata including sheet count, names, and total rows
  - Converts DataFrames to readable string format with column width limit (100 chars)

### `modules/standards_manager.py` (~380 lines) â­ NEW MODULE
- **StandardsManager**: National/local standards library management
- **Database**: SQLite (`data/standards.db`) with `StandardDocument` model
- **Storage**: Physical files saved to `data/standards/` directory

**Core Methods**:
1. `add_standard(uploaded_file, standard_name)`: Upload and process standard document
   - Calculates SHA256 hash to prevent duplicates
   - Parses document to extract content preview
   - Auto-extracts standard code (GB/JGJ/CJJ/DB patterns)
   - Categorizes into: å›½å®¶æ ‡å‡†/è¡Œä¸šæ ‡å‡†/åœ°æ–¹æ ‡å‡†
2. `get_all_standards(category)`: List all standards with optional filter
3. `get_standard_content(standard_id)`: Get full text content
4. `delete_standard(standard_id)`: Remove standard (file + DB record)
5. `search_standards(keyword)`: Search by code or name
6. `get_statistics()`: Summary counts by category

**Database Schema**:
```sql
CREATE TABLE standard_documents (
    id INTEGER PRIMARY KEY,
    standard_code VARCHAR(100) UNIQUE,  -- e.g. "GB50500-2013"
    standard_name VARCHAR(500),
    file_name VARCHAR(500),
    file_path VARCHAR(1000),
    file_hash VARCHAR(64),              -- SHA256
    file_size INTEGER,
    category VARCHAR(100),              -- å›½å®¶æ ‡å‡†/è¡Œä¸šæ ‡å‡†/åœ°æ–¹æ ‡å‡†
    content_preview TEXT,               -- First 500 chars
    upload_time DATETIME,
    update_time DATETIME
);
```

### `modules/ai_service.py` (412 lines)
- **ClaudeService**: Wraps Anthropic API client
- Default model: `claude-sonnet-4-20250514` (Sonnet 4)
- Alternative model: `claude-opus-4-20250514` (Opus 4 - higher quality, higher cost)

**Core Methods (New Architecture)**:
1. **`parse_bidding_document_structured(document_contents)`**
   - 7-category structured parsing
   - max_tokens: 16000
   - temperature: 0.2 (high accuracy)
   - Uses `BIDDING_DOCUMENT_ANALYSIS_PROMPT`

2. **`extract_evaluation_criteria(analysis_report)`**
   - Extract evaluation criteria from analysis report
   - max_tokens: 8000
   - temperature: 0.3
   - Uses `EVALUATION_CRITERIA_EXTRACTION_PROMPT`

3. **`generate_technical_proposal_outline(project_requirements, evaluation_criteria)`**
   - Generate JSON outline structure
   - max_tokens: 8000
   - temperature: 0.4
   - Uses `TECHNICAL_PROPOSAL_OUTLINE_PROMPT`

4. **`generate_technical_proposal_section(section_title, word_count, section_requirements, project_info, evaluation_criteria)`**
   - Generate individual sections
   - max_tokens: 8000
   - temperature: 0.5 (moderate creativity)
   - Uses `TECHNICAL_PROPOSAL_SECTION_PROMPT`

**Legacy Methods** (preserved for compatibility):
- `analyze_bidding_document()`: Old analysis method (not recommended)
- `generate_bidding_response()`: Old generation method (not recommended)
- `chat()`: Generic chat interface

### `modules/prompts.py` (405 lines) âœ¨ **NEW MODULE**
Centralized AI prompt template library using Python f-strings:

1. **`BIDDING_DOCUMENT_ANALYSIS_PROMPT`** (160 lines)
   - 7-category structured parsing template
   - Categories: åŸºç¡€ä¿¡æ¯, èµ„æ ¼è¦æ±‚, è¯„å®¡è¦æ±‚, æŠ•æ ‡æ–‡ä»¶è¦æ±‚, æ— æ•ˆæ ‡ä¸åºŸæ ‡é¡¹, åº”æ ‡éœ€æäº¤æ–‡ä»¶, æ‹›æ ‡æ–‡ä»¶å®¡æŸ¥
   - Includes risk identification (6 types) and fairness review (5 dimensions)
   - Output: Markdown format with clear hierarchy

2. **`EVALUATION_CRITERIA_EXTRACTION_PROMPT`** (53 lines)
   - Extract project requirements and evaluation criteria
   - Focus on "æ–½å·¥ç»„ç»‡è®¾è®¡" evaluation factors
   - Output: Markdown format with scoring details

3. **`TECHNICAL_PROPOSAL_OUTLINE_PROMPT`** (127 lines)
   - Generate technical proposal outline structure
   - Output: JSON format with word count suggestions
   - Reference structure: ä¸€ã€æŠ€æœ¯æ–‡ä»¶ / äºŒã€é¡¹ç›®ç»ç†è§†é¢‘é™ˆè¿°åŠç­”è¾© / ä¸‰ã€æ–½å·¥ç»„ç»‡è®¾è®¡

4. **`TECHNICAL_PROPOSAL_SECTION_PROMPT`** (55 lines)
   - Generate individual section content
   - Input: section title, word count, requirements, project info, evaluation criteria
   - Output: Markdown format, professional engineering language

### `modules/database.py` (150 lines)
- **BiddingRecord** ORM model with SQLAlchemy
- Fields: `project_name`, `uploaded_files` (JSON), `analysis_report`, `bidding_response`, `status`, `create_time`, `update_time`
- Status progression: draft â†’ analyzed â†’ completed
- `uploaded_files` stored as JSON string: `{"æ‹›æ ‡æ–‡ä»¶æ­£æ–‡": "filename.pdf", ...}`
- `bidding_response` field **actually stores evaluation criteria** (legacy naming for DB compatibility)
- Database auto-created at `data/bidding_system.db` on first run

## AI Prompt Engineering Strategy

### Modular Prompt Management
All prompts are centralized in `modules/prompts.py`:
- **Benefits**: Version control, easy maintenance, consistent formatting
- **Format**: Markdown output for easy parsing and display
- **Structure**: Clear hierarchical organization with numbered lists

### Multi-Step Generation Approach
Complex tasks are broken into sequential steps to improve accuracy:
1. **Parse** â†’ 7-category structured parsing (high accuracy, low creativity)
2. **Extract** â†’ Evaluation criteria extraction (moderate accuracy)
3. **Outline** â†’ Technical proposal outline (moderate creativity)
4. **Generate** â†’ Section-by-section content generation (higher creativity)

Each step builds on previous results, creating a pipeline of increasing specificity.

### Temperature Strategy
- **0.2** (parse): Maximize accuracy for information extraction
- **0.3** (extract): Balance accuracy and interpretation
- **0.4** (outline): Allow creative structuring
- **0.5** (generate): Enable professional writing with creativity

## Configuration

### Environment Variables (.env)
```bash
# Required
ANTHROPIC_API_KEY=sk-ant-api03-...

# Optional - For OpenRouter or custom endpoints
ANTHROPIC_BASE_URL=https://openrouter.ai/api/v1
ANTHROPIC_MODEL=anthropic/claude-sonnet-4

# Optional - System settings
MAX_FILE_SIZE_MB=50
UPLOAD_FOLDER=database
DATABASE_PATH=data/bidding_system.db
```

### Model Selection
To change Claude model, edit `modules/ai_service.py` line 49:
```python
self.model = "claude-opus-4-20250514"  # For higher quality
```

### Using OpenRouter
Configure in `.env`:
```bash
ANTHROPIC_BASE_URL=https://openrouter.ai/api/v1
ANTHROPIC_MODEL=anthropic/claude-sonnet-4
```
Code automatically detects `ANTHROPIC_BASE_URL` and switches to custom service.

## Common Development Tasks

### Adding Support for New File Format
1. Add parser method to `DocumentParser` class following signature:
   ```python
   def parse_newformat(self, file_path: str) -> Dict[str, str]:
       return {'content': str, 'metadata': dict}
   ```
2. Register in `supported_formats` dict in `__init__`
3. Add file extension to appropriate category in `file_categories` or create new category

### Adding New File Upload Category
Modify `file_categories` list in `app.py` `file_upload_tab()` function:
```python
file_categories.append({
    "name": "æ–°ç±»åˆ«åç§°",
    "types": ["pdf", "docx"],
    "category": "æ‹›æ ‡æ–‡ä»¶"  # or "æ‹›æ ‡æ–‡ä»¶é™„ä»¶"
})
```

### Modifying Analysis Categories
Edit `BIDDING_DOCUMENT_ANALYSIS_PROMPT` in `modules/prompts.py`:
- Add new category under the 7 main categories
- Add new subcategory under existing categories
- Modify extraction requirements for specific fields

### Customizing Technical Proposal Structure
Edit `TECHNICAL_PROPOSAL_OUTLINE_PROMPT` in `modules/prompts.py`:
- Modify reference outline structure
- Adjust suggested word counts
- Add new chapter templates

### Extending Database Schema
1. Add columns to `BiddingRecord` model in `modules/database.py`
2. Database auto-migrates on next run (SQLite limitation: some migrations may fail, may need to drop table)
3. Update `create_record` and `update_record` methods if needed

## Code Style Guidelines

### String Quoting for UI Messages
When UI messages contain quotation marks, use alternating single and double quotes to avoid syntax errors:
```python
# Correct: Use single quotes inside double-quoted strings
st.warning("âš ï¸ è¯·å…ˆåœ¨'æ–‡ä»¶ä¸Šä¼ 'æ ‡ç­¾é¡µä¸Šä¼ æ ‡ä¹¦æ–‡ä»¶")

# Incorrect: Nested double quotes cause syntax errors
st.warning("âš ï¸ è¯·å…ˆåœ¨"æ–‡ä»¶ä¸Šä¼ "æ ‡ç­¾é¡µä¸Šä¼ æ ‡ä¹¦æ–‡ä»¶")  # SyntaxError
```

### Session State Access Pattern
Always initialize session state keys before using them:
```python
if 'key_name' not in st.session_state:
    st.session_state.key_name = default_value
```

### File Path Handling
Use `os.path.join()` for cross-platform compatibility:
```python
file_path = os.path.join("database", filename)  # NOT "database/" + filename
```

## Important Notes

- **Python 3.9+** required
- Streamlit reruns entire script on user interaction - **use session state for persistence**
- Uploaded files persist in `database/` directory (NOT `uploads/`) - implement cleanup if needed
- Database session managed by `DatabaseManager` - no manual commit needed except in CRUD methods
- All 6 file categories are **intentionally optional** to accommodate varying bidding document structures
- Evaluation criteria are stored in `bidding_response` field (legacy naming for DB compatibility)

## Debugging Tips

### Common Issues

**Issue**: Session state lost after page reload
- **Cause**: Not saving to database
- **Fix**: Ensure `db_manager.update_record()` is called after critical operations

**Issue**: File parsing returns empty content
- **Cause**: Encoding issues or unsupported format
- **Fix**: Check file encoding, verify file is not password-protected

**Issue**: API call timeout
- **Cause**: max_tokens too high or document too long
- **Fix**: Reduce max_tokens or implement text truncation

**Issue**: JSON parsing fails for outline
- **Cause**: AI returned non-JSON format
- **Fix**: Code includes fallback to raw text mode (check `raw: True` flag)

## Performance Optimization

### Current Limitations
- Max tokens per API call: 16000 (for structured parsing)
- Document content truncation: 2000 chars per file (in old generation method)
- No caching for repeated API calls with same input

### Optimization Suggestions
1. **Implement API response caching** - Cache results for identical inputs
2. **Add document chunking** - For very long documents, split into chunks
3. **Use streaming API** - If supported, for better user experience
4. **Add database indexes** - On project_name, create_time for faster searches
5. **Consider async processing** - For batch operations

## Testing

### Environment Setup Test
```bash
python test_setup.py
```
Verifies:
- API key configuration
- Package dependencies
- Database connectivity

### Prompt Testing
```bash
python test_prompts.py
```
Tests individual prompt templates with sample data.

## Version History

### v1.0.0 (2026-01-15)
- âœ… Complete system architecture with 3-phase workflow
- âœ… 7-category structured parsing
- âœ… Automatic evaluation criteria extraction
- âœ… Step-by-step technical proposal generation
- âœ… Centralized prompt management (`modules/prompts.py`)
- âœ… 6 optional file upload categories
- âœ… Historical record management
- âœ… MD/TXT format export

## TODO & Future Enhancements

### High Priority
- [ ] Add Word/PDF export for technical proposals (currently only MD/TXT)
- [ ] Implement API response caching to reduce costs
- [ ] Add document chunking for very long bidding documents

### Medium Priority
- [ ] Batch project processing functionality
- [ ] Technical proposal template library
- [ ] Section content editing functionality
- [ ] Evaluation criteria visualization

### Low Priority
- [ ] User authentication and permissions
- [ ] Multi-user collaboration features
- [ ] Support for CAD drawing text extraction
- [ ] Comparison analysis between multiple bidding documents

---

**Maintained by**: Yecheng Zhang & Claude Code
**Last Updated**: 2026-01-15
**License**: Internal Use
